{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to calculate grid adjacency and do p-class clustering for Rosi's MPAS aquaplanet grid.\n",
    "\n",
    "Also writes out a single time step of p-class values.\n",
    "\n",
    "James Ruppert  \n",
    "9/1/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc, colors\n",
    "import cartopy.crs as ccrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_write = True\n",
    "do_write = False\n",
    "\n",
    "pclass_names = ['DC', 'CG', 'SC', 'ST', 'AN', 'DSA']\n",
    "nclass = len(pclass_names)\n",
    "exp_names = [\"CTL\", \"HOMO_RAD\", \"CLIM_RAD\"]\n",
    "expName=exp_names[0]\n",
    "\n",
    "pickle_dir = f\"/glade/derecho/scratch/ruppert/tc-crfrad/pickle_out\"\n",
    "# pickle_dir = f\"../../../pickle_out/aquaplanet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read/Write variable from MPAS output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read grid data\n",
    "do_write_grid = False\n",
    "grid_pickle_file = f\"{pickle_dir}/grid_data.pickle\"\n",
    "if do_write_grid:\n",
    "    import xarray as xr\n",
    "    grid_path = \"/glade/work/rberrios/MPAS/aqua_sstmax10N_ASD/plus4K/TC_3km/x5.tropical_3km_10N.init.nc\"\n",
    "    grid = xr.open_dataset(grid_path)\n",
    "    areaCell = grid.areaCell\n",
    "    latCell = np.degrees(grid.latCell)\n",
    "    lonCell = np.degrees(grid.lonCell)\n",
    "    with open(grid_pickle_file, 'wb') as f:\n",
    "        pickle.dump([areaCell, latCell, lonCell], f)\n",
    "else:\n",
    "    with open(grid_pickle_file, 'rb') as f:\n",
    "        areaCell, latCell, lonCell = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-time jobs\n",
    "\n",
    "if do_write:\n",
    "\n",
    "    import xarray as xr\n",
    "    import dask.array as da\n",
    "    import dask\n",
    "    from precip_class_mpas import *\n",
    "\n",
    "    model_path = \"/glade/campaign/mmm/dpm/rberrios/glade_scratch/MPAS_APE/aqua_sstmax10N_ASD/\"\n",
    "\n",
    "    # Get list of desired file times\n",
    "    file_times_arr = np.arange('2000-05-01T06:00:00', '2000-05-11T06:00:00', 6, dtype='datetime64[h]')\n",
    "    file_times = [file_times_arr[i].astype('datetime64[D]').astype(str)+'_'+str(file_times_arr[i]).split('T')[1].split(':')[0]+'.00.00' for i in range(len(file_times_arr))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible looping over time\n",
    "\n",
    "if do_write:\n",
    "\n",
    "    for it in range(0,len(file_times),5):\n",
    "        \n",
    "        time = file_times[it]\n",
    "        # if it == 20:\n",
    "        #     continue\n",
    "\n",
    "        # Choose experiment and read in model data\n",
    "        data_path = f\"{model_path}{expName}/TC_3km/\"\n",
    "        wp_files = data_path+'waterPaths.'+time+'.nc'\n",
    "        nCells_chunk_size = 100000\n",
    "        ds = xr.open_mfdataset(wp_files,\n",
    "                    parallel=True, \n",
    "                    chunks={\"Time\": -1, \"nCells\": nCells_chunk_size})\n",
    "\n",
    "        q_int_dask = da.stack([\n",
    "            ds.lwp.data,\n",
    "            ds.iwp.data,\n",
    "            ds.rwp.data,\n",
    "            ds.gwp.data\n",
    "        ], axis=0) # Stack along a new 0th dimension for the different water paths\n",
    "\n",
    "        c_type_dask = precip_class_mpas(q_int_dask)\n",
    "        c_type = dask.compute(c_type_dask)[0][0] # dask.compute returns a tuple of results\n",
    "\n",
    "        # Write to pickle\n",
    "        pickle_file_out = f\"{pickle_dir}/pclass_{expName}_{time}.pickle\"\n",
    "        with open(pickle_file_out, 'wb') as f:\n",
    "            # pickle.dump(PE_thisExp, f)\n",
    "            # pickle.dump([latCell, lonCell, c_type], f)\n",
    "            pickle.dump(c_type, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read list of files from a directory\n",
    "# import glob\n",
    "# file_list = glob.glob(f\"{pickle_dir}/pclass_{expName}*pickle\")\n",
    "\n",
    "# Get list of desired file times\n",
    "file_times_arr = np.arange('2000-05-01T06:00:00', '2000-05-11T06:00:00', 6, dtype='datetime64[h]')\n",
    "file_times = [file_times_arr[i].astype('datetime64[D]').astype(str)+'_'+str(file_times_arr[i]).split('T')[1].split(':')[0]+'.00.00' for i in range(len(file_times_arr))]\n",
    "file_times_read = [file_times[i] for i in range(0,len(file_times),5)]\n",
    "\n",
    "# time = file_times[20]\n",
    "\n",
    "# Write to pickle\n",
    "# Read from pickle\n",
    "c_type = []\n",
    "for time in file_times_read:\n",
    "    pickle_file_read = f\"{pickle_dir}/pclass_{expName}_{time}.pickle\"\n",
    "    with open(pickle_file_read, 'rb') as f:\n",
    "        c_type.append(pickle.load(f))\n",
    "c_type = np.array(c_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot P-Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_bounds = (0,60,0,30)\n",
    "filter = np.where((lonCell >= ll_bounds[0]) &\n",
    "                  (lonCell <= ll_bounds[1]) &\n",
    "                  (latCell >= ll_bounds[2]) &\n",
    "                  (latCell <= ll_bounds[3]))[0]\n",
    "\n",
    "font = {'family' : 'sans-serif',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 12}\n",
    "rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = np.array([-0.5, .5, 1.5, 2.5, 3.5, 4.5, 5.5])\n",
    "norm = colors.BoundaryNorm(boundaries=bounds, ncolors=6)\n",
    "\n",
    "nlevs = len(bounds) - 1\n",
    "cmap = plt.get_cmap('Accent_r', nlevs)\n",
    "\n",
    "for it, itime in enumerate(file_times_read):\n",
    "\n",
    "    fig = plt.figure(figsize=(12,10))\n",
    "    fig.set_facecolor('white')\n",
    "    ax = fig.add_subplot(111, projection=ccrs.Mollweide())\n",
    "    ax.gridlines()#draw_labels=True, x_inline=False, y_inline=False)#, dms=True)\n",
    "\n",
    "    # sc = ax.scatter(lonCell[filter], latCell[filter], c=c_type[filter], cmap=cmap, norm=norm, s=1)\n",
    "    sc = ax.scatter(lonCell, latCell, c=c_type[it], cmap=cmap, norm=norm, s=1, transform=ccrs.PlateCarree())\n",
    "    # sc = ax.contourf(lonCell, latCell, c=c_type, cmap=cmap, norm=norm, s=1)\n",
    "    # sc = ax.tricontourf(lonCell, latCell, c_type, levels=15, cmap='viridis')\n",
    "\n",
    "    cbar=plt.colorbar(sc, fraction=0.03, pad=0.02, ticks=(0,1,2,3,4,5), shrink=0.65)\n",
    "    cbar.set_ticklabels(['Nonraining','Deep', 'Congestus', 'Shallow', 'Stratiform', 'Anvil'])\n",
    "    cbar.ax.tick_params(labelsize=14)\n",
    "    ax.set_title(f\"Cloud Classification ({expName}, {itime})\", fontsize=16)\n",
    "    # plt.ylim((-5,25))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(f\"pclass_map_{expName}_{itime}.png\", dpi=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plotting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
