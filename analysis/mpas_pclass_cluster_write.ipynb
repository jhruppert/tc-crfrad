{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to calculate grid adjacency and do p-class clustering for Rosi's MPAS aquaplanet grid.\n",
    "\n",
    "James Ruppert  \n",
    "9/1/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "import pickle\n",
    "import dask\n",
    "from dask_jobqueue import PBSCluster\n",
    "from dask.distributed import Client\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from precip_class_mpas import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_write_adj = False\n",
    "# do_write_adj = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/glade/derecho/scratch/ruppert/tc-crfrad/pickle_out/'\n",
    "adj_pickle_file = data_path+'../adjacency_matrix.pkl'\n",
    "\n",
    "test_names = [\"CTL\",\"HOMO_RAD\",\"CLIM_RAD\"]\n",
    "# test_names = [\"CTL\",\"HOMO_RAD\"]\n",
    "ntest = len(test_names)\n",
    "pclass_names = ['DC', 'CG', 'SC', 'ST', 'AN', 'DSA']\n",
    "pclass_names_long = ['DeepC', 'Congest', 'Shallow', 'Stratiform', 'Anvil', 'DSA']\n",
    "nclass = len(pclass_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start cluster and scale it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "\n",
      "#PBS -N dask-wk23-hpc\n",
      "#PBS -q casper\n",
      "#PBS -A UOKL0049\n",
      "#PBS -l select=1:ncpus=1:mem=10GB\n",
      "#PBS -l walltime=12:00:00\n",
      "#PBS -e /glade/derecho/scratch/ruppert/dask//\n",
      "#PBS -o /glade/derecho/scratch/ruppert/dask//\n",
      "\n",
      "/glade/work/ruppert/conda-envs/plotting/bin/python -m distributed.cli.dask_worker tcp://128.117.211.221:46497 --name dummy-name --nthreads 1 --memory-limit 10.00GiB --nanny --death-timeout 60 --local-directory /glade/derecho/scratch/ruppert/dask/ --interface ext\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/ruppert/conda-envs/plotting/lib/python3.11/site-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 45941 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from ncar_jobqueue import NCARCluster\n",
    "# cluster = PBSCluster()\n",
    "# cluster.scale(4 * 9) # Ask for 4 x 9 workers\n",
    "# cluster\n",
    "cluster = PBSCluster(\n",
    "    job_name = 'dask-wk23-hpc',\n",
    "    cores = 1,\n",
    "    processes = 1,\n",
    "    memory = '10GiB',\n",
    "    account = 'UOKL0049',\n",
    "    log_directory = '/glade/derecho/scratch/ruppert/dask/',\n",
    "    local_directory = '/glade/derecho/scratch/ruppert/dask/',\n",
    "    resource_spec = 'select=1:ncpus=1:mem=10GB',\n",
    "    queue = 'casper',\n",
    "    # queue = 'main',\n",
    "    walltime = '12:00:00',\n",
    "    interface = 'ext'\n",
    ")\n",
    "print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-44ca0bc4-8a97-11f0-b6ff-ac1f6bc7cc9a</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> dask_jobqueue.PBSCluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"http://128.117.211.221:45941/status\" target=\"_blank\">http://128.117.211.221:45941/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\">\n",
       "    </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">PBSCluster</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">24fe80f6</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard:</strong> <a href=\"http://128.117.211.221:45941/status\" target=\"_blank\">http://128.117.211.221:45941/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Workers:</strong> 0\n",
       "                </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total threads:</strong> 0\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total memory:</strong> 0 B\n",
       "                </td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "\n",
       "        <details>\n",
       "            <summary style=\"margin-bottom: 20px;\">\n",
       "                <h3 style=\"display: inline;\">Scheduler Info</h3>\n",
       "            </summary>\n",
       "\n",
       "            <div style=\"\">\n",
       "    <div>\n",
       "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
       "        <div style=\"margin-left: 48px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-166aff9d-3813-4f15-97cd-9c025b8f192f</p>\n",
       "            <table style=\"width: 100%; text-align: left;\">\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Comm:</strong> tcp://128.117.211.221:46497\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Workers:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard:</strong> <a href=\"http://128.117.211.221:45941/status\" target=\"_blank\">http://128.117.211.221:45941/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total threads:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Started:</strong> Just now\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total memory:</strong> 0 B\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <details style=\"margin-left: 48px;\">\n",
       "        <summary style=\"margin-bottom: 20px;\">\n",
       "            <h3 style=\"display: inline;\">Workers</h3>\n",
       "        </summary>\n",
       "\n",
       "        \n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "\n",
       "        </details>\n",
       "    </div>\n",
       "</div>\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tcp://128.117.211.221:46497' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client(cluster) # Connect this local process to remote workers\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the cluster to n workers (which will use n jobs here)\n",
    "ncpu = 36\n",
    "# ncpu = 24\n",
    "# ncpu = 10\n",
    "# ncpu = 18\n",
    "cluster.scale(ncpu)\n",
    "\n",
    "# Block progress until workers have spawned (typically only in demos and benchmarks!)\n",
    "client.wait_for_workers(ncpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set paths, read initial conditions, find tropical indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_path = \"/glade/work/rberrios/MPAS/aqua_sstmax10N_ASD/plus4K/TC_3km/x5.tropical_3km_10N.init.nc\"\n",
    "grid = xr.open_dataset(grid_path)\n",
    "\n",
    "latCell = np.degrees(grid.latCell)\n",
    "lonCell = np.degrees(grid.lonCell)\n",
    "\n",
    "# #find indexes within desired latitudinal range\n",
    "# latbounds = [15, 20.0]\n",
    "# # latbounds = [0, 15.0]\n",
    "# ind_within_lat = np.where( (latCell >= latbounds[0]) & (latCell <= latbounds[1]) )[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and write adjacency mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_write_adj:\n",
    "\n",
    "    from scipy.sparse import csr_matrix\n",
    "\n",
    "    ##### Read grid data\n",
    "\n",
    "    # grid_tropical = grid.isel(nCells=ind_within_lat)\n",
    "    # areaCell = grid.areaCell\n",
    "    neighbor_cells = grid.cellsOnCell\n",
    "    ncell_edges = grid.nEdgesOnCell\n",
    "    # len(ind_within_lat)\n",
    "\n",
    "    ##### Build adjacency matrix\n",
    "\n",
    "    nCells, maxEdges = neighbor_cells.data.shape\n",
    "\n",
    "    # Convert to 0-based neighbors, set padded 0s â†’ -1\n",
    "    neighbors = neighbor_cells.data - 1\n",
    "    neighbors = da.where(neighbors < 0, -1, neighbors)\n",
    "\n",
    "    # Repeat row indices to align with neighbors\n",
    "    rows = da.repeat(da.arange(nCells), maxEdges)\n",
    "    cols = neighbors.reshape(-1)\n",
    "\n",
    "    # Filter valid neighbors\n",
    "    valid = cols >= 0\n",
    "    rows = rows[valid]\n",
    "    cols = cols[valid]\n",
    "\n",
    "    # Compute once here (bring to NumPy)\n",
    "    rows_np, cols_np = da.compute(rows, cols)\n",
    "\n",
    "    # Build adjacency\n",
    "    data = np.ones(len(rows_np), dtype=int)\n",
    "    adj_full = csr_matrix((data, (rows_np, cols_np)), shape=(nCells, nCells))\n",
    "\n",
    "    # Save adjacency as pickle\n",
    "    with open(adj_pickle_file, 'wb') as f:\n",
    "        pickle.dump(adj_full, f)\n",
    "\n",
    "else:\n",
    "\n",
    "    with open(adj_pickle_file, 'rb') as f:\n",
    "        adj_full = pickle.load(f)\n",
    "    # adj_subset = adj_full[ind_within_lat[:, None], ind_within_lat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnose clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With help from ChatGPT\n",
    "\n",
    "def mpas_label_from_adj(\n",
    "    c_type_flat,\n",
    "    adj_full,\n",
    "    mask_condition=None,\n",
    "    subset_indices=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Connected component labeling on MPAS using a precomputed adjacency (adj_full).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    c_type_flat : dask.array or np.ndarray, shape (nCells,)\n",
    "        Field for masking (e.g., c_type).\n",
    "    adj_full : scipy.sparse.csr_matrix\n",
    "        Full adjacency matrix for all MPAS cells.\n",
    "    mask_condition : callable or None\n",
    "        Function f(arr) -> bool array, defining which cells are \"active\".\n",
    "        If None, all cells are considered active.\n",
    "    subset_indices : array-like of int or None\n",
    "        Optional subset of cells to restrict analysis to.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    labels_out : np.ndarray\n",
    "        Label array (background = 0). Length = nCells if no subset,\n",
    "        else length = len(subset_indices).\n",
    "    num_features : int\n",
    "        Number of connected components found.\n",
    "    \"\"\"\n",
    "\n",
    "    # Mask condition\n",
    "    if mask_condition is None:\n",
    "        mask = da.ones_like(c_type_flat, dtype=bool)\n",
    "    else:\n",
    "        mask = mask_condition(c_type_flat)\n",
    "\n",
    "    # Compute mask into NumPy\n",
    "    mask_np = mask.compute() if isinstance(mask, da.Array) else np.asarray(mask)\n",
    "\n",
    "    if subset_indices is not None:\n",
    "        subset_indices = (\n",
    "            subset_indices.compute()\n",
    "            if isinstance(subset_indices, da.Array)\n",
    "            else np.asarray(subset_indices, dtype=int)\n",
    "        )\n",
    "        # mask_np = mask_np[subset_indices]\n",
    "        adj = adj_full[subset_indices, :][:, subset_indices]\n",
    "    else:\n",
    "        adj = adj_full\n",
    "\n",
    "    # Keep only active nodes\n",
    "    active = np.where(mask_np)[0]\n",
    "    adj_masked = adj[active, :][:, active]\n",
    "\n",
    "    # Connected components\n",
    "    num_features, labels_active = connected_components(adj_masked, directed=False)\n",
    "\n",
    "    # Remap back to full (subset) length\n",
    "    labels_out = np.zeros(mask_np.shape[0], dtype=int)\n",
    "    labels_out[active] = labels_active + 1  # +1 to match ndimage.label\n",
    "\n",
    "    # labels_out is length nCells (or len(subset)), background = 0\n",
    "    sizes = np.bincount(labels_out)[1:]  # skip label 0 (background)\n",
    "\n",
    "    return labels_out, num_features, sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main driver loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get file list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of desired file times\n",
    "file_times_arr = np.arange('2000-05-01T06:00:00', '2000-05-11T06:00:00', 6, dtype='datetime64[h]')\n",
    "file_times = [file_times_arr[i].astype('datetime64[D]').astype(str)+'_'+str(file_times_arr[i]).split('T')[1].split(':')[0]+'.00.00' for i in range(len(file_times_arr))]\n",
    "\n",
    "istart_set=11\n",
    "# file_times[istart_set]\n",
    "# file_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# Main loop\n",
    "\n",
    "print('Starting loop...')\n",
    "nCells_chunk_size = 100000\n",
    "\n",
    "exp_names = [\"CTL\", \"HOMO_RAD\", \"CLIM_RAD\"]\n",
    "\n",
    "main_path = \"/glade/campaign/mmm/dpm/rberrios/glade_scratch/MPAS_APE/aqua_sstmax10N_ASD/\"\n",
    "\n",
    "for expName in exp_names:\n",
    "# for expName in exp_names[:1]:\n",
    "\n",
    "    data_path = f\"{main_path}{expName}/TC_3km/\"\n",
    "    pickle_dir = f\"/glade/derecho/scratch/ruppert/tc-crfrad/mpas/{expName}\"\n",
    "\n",
    "    # Open the dataset with dask backend. This loads lazily.\n",
    "    # Specify chunks to optimize memory usage and parallel processing.\n",
    "    # You'll need to know typical chunk sizes for your variables, or let xarray guess.\n",
    "    # For large datasets, manual chunking can be critical.\n",
    "    # Example: If 'Time' dimension is large, chunk it. 'nCells' might be good to chunk too.\n",
    "    # ds = xr.open_mfdataset(data_path + \"waterPaths*\", combine=\"nested\", concat_dim=\"Time\",\n",
    "    #                        chunks={'Time': 'auto', 'nCells': 'auto'}) # 'auto' lets Dask guess\n",
    "    # Or specify explicitly, e.g., {'Time': 24, 'nCells': 1000}\n",
    "\n",
    "    if expName == \"CTL\":\n",
    "        istart = 0\n",
    "    elif expName == \"HOMO_RAD\":\n",
    "        istart = 0\n",
    "    elif expName == \"CLIM_RAD\":\n",
    "        istart = 0\n",
    "        # istart = istart_set\n",
    "\n",
    "    # for time in file_times:\n",
    "    for time in file_times[istart:]:\n",
    "    # for time in file_times[0:1]:\n",
    "\n",
    "        # print(f\"Processing {expName} for time {time}\")\n",
    "        print(f\"Processing file: pclass_cluster_{expName}_{time}.pickle\")\n",
    "\n",
    "        # print('opening files')\n",
    "        # wp_files = [data_path+'waterPaths.'+time+'.nc' for time in file_times]\n",
    "        wp_files = data_path+'waterPaths.'+time+'.nc'\n",
    "        # ds = xr.open_mfdataset(wp_files,\n",
    "        ds = xr.open_mfdataset(wp_files,\n",
    "                    # combine=\"nested\", concat_dim=\"Time\", \n",
    "                    parallel=True, \n",
    "                    chunks={\"Time\": -1, \"nCells\": nCells_chunk_size})\n",
    "\n",
    "        # print()\n",
    "        # print('subsetting')\n",
    "        # Select cells within latitude range. This operation is also lazy if `ds` is Dask-backed.\n",
    "        # ds_tropical = ds.isel(nCells=ind_within_lat)\n",
    "        ds_tropical = ds\n",
    "\n",
    "        # print('reading variables')\n",
    "        # Convert to a list of DataArrays, and wrap in dask.array.stack to create a single Dask array\n",
    "        # This creates a Dask-backed array 'q_int_dask' without loading data into memory yet.\n",
    "        q_int_dask = da.stack([\n",
    "            ds_tropical.lwp.data,\n",
    "            ds_tropical.iwp.data,\n",
    "            ds_tropical.rwp.data,\n",
    "            ds_tropical.gwp.data\n",
    "        ], axis=0) # Stack along a new 0th dimension for the different water paths\n",
    "\n",
    "        # print('classifying')\n",
    "        # Call the classification function. This will return a Dask array (c_type_dask).\n",
    "        # The actual computation of c_type is still lazy at this point.\n",
    "        c_type_dask = precip_class_mpas(q_int_dask)\n",
    "\n",
    "        # Now, use dask.compute to run all these sums in parallel\n",
    "        # This is the point where the data will actually be loaded and processed by Dask workers.\n",
    "        # This avoids loading the full `c_type` array into memory at once.\n",
    "        results = dask.compute(c_type_dask)[0] # dask.compute returns a tuple of results\n",
    "\n",
    "        labels_write = []\n",
    "        nfeat_write = []\n",
    "        sizes_write = []\n",
    "        for iclass in range(nclass):\n",
    "\n",
    "            if iclass < (nclass-1):\n",
    "                mask_condition = lambda arr, iclass=iclass: arr == (iclass + 1)\n",
    "            else:\n",
    "                mask_condition = lambda arr: (arr == 1) | (arr >= 4)\n",
    "\n",
    "            labels, nfeat, sizes = mpas_label_from_adj(\n",
    "                c_type_dask[0],\n",
    "                adj_full,\n",
    "                mask_condition=mask_condition,\n",
    "                subset_indices=None\n",
    "                )\n",
    "            labels_write.append(labels)\n",
    "            nfeat_write.append(nfeat)\n",
    "            sizes_write.append(sizes)\n",
    "\n",
    "        # Write out to pickle\n",
    "\n",
    "        pickle_file_out = f\"{pickle_dir}/pclass_cluster_{expName}_{time}.pickle\"\n",
    "        with open(pickle_file_out, 'wb') as f:\n",
    "            # pickle.dump(PE_thisExp, f)\n",
    "            pickle.dump([labels_write, nfeat_write, sizes_write], f)\n",
    "\n",
    "    print(f\"Finished processing {expName}\")\n",
    "\n",
    "print('Classification complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plotting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
