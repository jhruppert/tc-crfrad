{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to calculate grid adjacency and do p-class clustering for Rosi's MPAS aquaplanet grid.\n",
    "\n",
    "James Ruppert  \n",
    "9/1/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "import pickle\n",
    "import dask\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_write_adj = False\n",
    "# do_write_adj = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/glade/derecho/scratch/ruppert/tc-crfrad/pickle_out/'\n",
    "pickle_file = data_path+'adjacency_matrix.pkl'\n",
    "\n",
    "test_names = [\"CTL\",\"HOMO_RAD\",\"CLIM_RAD\"]\n",
    "# test_names = [\"CTL\",\"HOMO_RAD\"]\n",
    "ntest = len(test_names)\n",
    "pclass_names = ['DC', 'CG', 'SC', 'ST', 'AN', 'DSA']\n",
    "pclass_names_long = ['DeepC', 'Congest', 'Shallow', 'Stratiform', 'Anvil', 'DSA']\n",
    "nclass = len(pclass_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start cluster and scale it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "\n",
      "#PBS -N dask-wk23-hpc\n",
      "#PBS -q casper\n",
      "#PBS -A UOKL0049\n",
      "#PBS -l select=1:ncpus=1:mem=10GB\n",
      "#PBS -l walltime=12:00:00\n",
      "#PBS -e /glade/derecho/scratch/ruppert/dask//\n",
      "#PBS -o /glade/derecho/scratch/ruppert/dask//\n",
      "\n",
      "/glade/work/ruppert/conda-envs/plotting/bin/python -m distributed.cli.dask_worker tcp://128.117.211.222:44157 --name dummy-name --nthreads 1 --memory-limit 10.00GiB --nanny --death-timeout 60 --local-directory /glade/derecho/scratch/ruppert/dask/ --interface ext\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/ruppert/conda-envs/plotting/lib/python3.11/site-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 44551 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from ncar_jobqueue import NCARCluster\n",
    "# cluster = PBSCluster()\n",
    "# cluster.scale(4 * 9) # Ask for 4 x 9 workers\n",
    "# cluster\n",
    "\n",
    "from dask_jobqueue import PBSCluster\n",
    "cluster = PBSCluster(\n",
    "    job_name = 'dask-wk23-hpc',\n",
    "    cores = 1,\n",
    "    processes = 1,\n",
    "    memory = '10GiB',\n",
    "    account = 'UOKL0049',\n",
    "    log_directory = '/glade/derecho/scratch/ruppert/dask/',\n",
    "    local_directory = '/glade/derecho/scratch/ruppert/dask/',\n",
    "    resource_spec = 'select=1:ncpus=1:mem=10GB',\n",
    "    queue = 'casper',\n",
    "    # queue = 'main',\n",
    "    walltime = '12:00:00',\n",
    "    interface = 'ext'\n",
    ")\n",
    "print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-0adde70d-8852-11f0-bec2-ac1f6bc7cc7e</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> dask_jobqueue.PBSCluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"http://128.117.211.222:44551/status\" target=\"_blank\">http://128.117.211.222:44551/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\">\n",
       "    </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">PBSCluster</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">93d8f9a2</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard:</strong> <a href=\"http://128.117.211.222:44551/status\" target=\"_blank\">http://128.117.211.222:44551/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Workers:</strong> 0\n",
       "                </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total threads:</strong> 0\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total memory:</strong> 0 B\n",
       "                </td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "\n",
       "        <details>\n",
       "            <summary style=\"margin-bottom: 20px;\">\n",
       "                <h3 style=\"display: inline;\">Scheduler Info</h3>\n",
       "            </summary>\n",
       "\n",
       "            <div style=\"\">\n",
       "    <div>\n",
       "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
       "        <div style=\"margin-left: 48px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-5ab9a336-1cf1-4ef5-b9b1-fa5a432797bf</p>\n",
       "            <table style=\"width: 100%; text-align: left;\">\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Comm:</strong> tcp://128.117.211.222:44157\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Workers:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard:</strong> <a href=\"http://128.117.211.222:44551/status\" target=\"_blank\">http://128.117.211.222:44551/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total threads:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Started:</strong> Just now\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total memory:</strong> 0 B\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <details style=\"margin-left: 48px;\">\n",
       "        <summary style=\"margin-bottom: 20px;\">\n",
       "            <h3 style=\"display: inline;\">Workers</h3>\n",
       "        </summary>\n",
       "\n",
       "        \n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "\n",
       "        </details>\n",
       "    </div>\n",
       "</div>\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tcp://128.117.211.222:44157' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(cluster) # Connect this local process to remote workers\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m cluster\u001b[38;5;241m.\u001b[39mscale(ncpu)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Block progress until workers have spawned (typically only in demos and benchmarks!)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mncpu\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/work/ruppert/conda-envs/plotting/lib/python3.11/site-packages/distributed/client.py:1680\u001b[0m, in \u001b[0;36mClient.wait_for_workers\u001b[0;34m(self, n_workers, timeout)\u001b[0m\n\u001b[1;32m   1675\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1676\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`n_workers` must be a positive integer. Instead got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_workers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1677\u001b[0m     )\n\u001b[1;32m   1679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwait_for_workers\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1680\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcluster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_workers, n_workers, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "File \u001b[0;32m/glade/work/ruppert/conda-envs/plotting/lib/python3.11/site-packages/distributed/deploy/cluster.py:636\u001b[0m, in \u001b[0;36mCluster.wait_for_workers\u001b[0;34m(self, n_workers, timeout)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(n_workers, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m n_workers \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`n_workers` must be a positive integer. Instead got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_workers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    635\u001b[0m     )\n\u001b[0;32m--> 636\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/work/ruppert/conda-envs/plotting/lib/python3.11/site-packages/distributed/utils.py:363\u001b[0m, in \u001b[0;36mSyncMethodMixin.sync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/work/ruppert/conda-envs/plotting/lib/python3.11/site-packages/distributed/utils.py:436\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m e\u001b[38;5;241m.\u001b[39mis_set():\n\u001b[0;32m--> 436\u001b[0m         \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "File \u001b[0;32m/glade/work/ruppert/conda-envs/plotting/lib/python3.11/site-packages/distributed/utils.py:425\u001b[0m, in \u001b[0;36msync.<locals>.wait\u001b[0;34m(timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 425\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    427\u001b[0m         loop\u001b[38;5;241m.\u001b[39madd_callback(cancel)\n",
      "File \u001b[0;32m/glade/work/ruppert/conda-envs/plotting/lib/python3.11/threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    627\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/glade/work/ruppert/conda-envs/plotting/lib/python3.11/threading.py:331\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 331\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Scale the cluster to n workers (which will use n jobs here)\n",
    "ncpu = 36\n",
    "# ncpu = 24\n",
    "# ncpu = 10\n",
    "# ncpu = 18\n",
    "cluster.scale(ncpu)\n",
    "\n",
    "# Block progress until workers have spawned (typically only in demos and benchmarks!)\n",
    "client.wait_for_workers(ncpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set paths, read initial conditions, find tropical indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_path = \"/glade/work/rberrios/MPAS/aqua_sstmax10N_ASD/plus4K/TC_3km/x5.tropical_3km_10N.init.nc\"\n",
    "grid = xr.open_dataset(grid_path)\n",
    "\n",
    "latCell = np.degrees(grid.latCell)\n",
    "lonCell = np.degrees(grid.lonCell)\n",
    "\n",
    "# #find indexes within desired latitudinal range\n",
    "# latbounds = [15, 20.0]\n",
    "# # latbounds = [0, 15.0]\n",
    "# ind_within_lat = np.where( (latCell >= latbounds[0]) & (latCell <= latbounds[1]) )[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and write adjacency mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_write_adj:\n",
    "\n",
    "    from scipy.sparse import csr_matrix\n",
    "\n",
    "    ##### Read grid data\n",
    "\n",
    "    # grid_tropical = grid.isel(nCells=ind_within_lat)\n",
    "    # areaCell = grid.areaCell\n",
    "    neighbor_cells = grid.cellsOnCell\n",
    "    ncell_edges = grid.nEdgesOnCell\n",
    "    # len(ind_within_lat)\n",
    "\n",
    "    ##### Build adjacency matrix\n",
    "\n",
    "    nCells, maxEdges = neighbor_cells.data.shape\n",
    "\n",
    "    # Convert to 0-based neighbors, set padded 0s â†’ -1\n",
    "    neighbors = neighbor_cells.data - 1\n",
    "    neighbors = da.where(neighbors < 0, -1, neighbors)\n",
    "\n",
    "    # Repeat row indices to align with neighbors\n",
    "    rows = da.repeat(da.arange(nCells), maxEdges)\n",
    "    cols = neighbors.reshape(-1)\n",
    "\n",
    "    # Filter valid neighbors\n",
    "    valid = cols >= 0\n",
    "    rows = rows[valid]\n",
    "    cols = cols[valid]\n",
    "\n",
    "    # Compute once here (bring to NumPy)\n",
    "    rows_np, cols_np = da.compute(rows, cols)\n",
    "\n",
    "    # Build adjacency\n",
    "    data = np.ones(len(rows_np), dtype=int)\n",
    "    adj_full = csr_matrix((data, (rows_np, cols_np)), shape=(nCells, nCells))\n",
    "\n",
    "    # Save adjacency as pickle\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        pickle.dump(adj_full, f)\n",
    "\n",
    "else:\n",
    "\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        adj_full = pickle.load(f)\n",
    "    # adj_subset = adj_full[ind_within_lat[:, None], ind_within_lat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Column-based precipitation classification algorithm designed for application on\n",
    "# numerical model output.\n",
    "# \n",
    "# It has been designed using WRF model output using the Thompson and Eidhammer\n",
    "# (2014, JAS) microphysics scheme, which has 2 liquid and 3 frozen categories as\n",
    "# listed and expected below.\n",
    "# \n",
    "# Input:\n",
    "# \n",
    "#       Q_INT: n-D array of vertically integrated hydrometeors as f(q, X), where\n",
    "#               q(5) is the hydrometeor dimension, arranged as\n",
    "#               ['QCLOUD', 'QRAIN', 'QICE', 'QSNOW', 'QGRAUP'] and X includes the\n",
    "#               remaining (time and) spatial dimensions.\n",
    "# Returns:\n",
    "# \n",
    "#       C_TYPE: (n-2)-D array as f(X) with classification results:\n",
    "#               0: non-cloud\n",
    "#           Convective:\n",
    "#               1: deep convective\n",
    "#               2: congestus\n",
    "#               3: shallow\n",
    "#           Layered:\n",
    "#               4: stratiform\n",
    "#               5: anvil (weaker rainfall)\n",
    "# \n",
    "# Emily Luschen - emily.w.luschen-1@ou.edu\n",
    "# James Ruppert - jruppert@ou.edu\n",
    "# 5/19/23\n",
    "# Rosi RB - modified to ingest water paths and to return dask arrays\n",
    "\n",
    "def precip_class(q_int):\n",
    "    shape = q_int.shape\n",
    "    ndims=len(shape)\n",
    "    shape_out = shape[1:ndims]\n",
    "\n",
    "    # Integrated water variables\n",
    "    # Ensure these are Dask arrays if q_int is a Dask array\n",
    "    LWP = q_int[0]\n",
    "    IWP = q_int[1]\n",
    "    # For Q_INT input, q_int[2] is QICE, q_int[3] is QSNOW, q_int[4] is QGRAUP\n",
    "    # Your original code used q_int[2] for rain, q_int[3] for graupel.\n",
    "    # Make sure this indexing is consistent with your actual 'q_int' structure.\n",
    "    # Based on the function docstring: ['QCLOUD', 'QRAIN', 'QICE', 'QSNOW', 'QGRAUP']\n",
    "    # And your q_int = np.array([ds.lwp, ds.iwp, ds.rwp, ds.gwp]), this means:\n",
    "    # q_int[0] = lwp (QCLOUD + QRAIN combined in your data?)\n",
    "    # q_int[1] = iwp (QICE + QSNOW + QGRAUP combined in your data?)\n",
    "    # q_int[2] = rwp (QRAIN from your data)\n",
    "    # q_int[3] = gwp (QGRAUP from your data)\n",
    "    # So, the original function's `q_int[2]` (for rain_thresh) is `rwp` (your q_int[2])\n",
    "    # and `q_int[3]` (for graup_thresh) is `gwp` (your q_int[3]).\n",
    "    # This seems consistent, just making sure the comments are aligned.\n",
    "\n",
    "    RWP = q_int[2] # Rain Water Path\n",
    "    GWP = q_int[3] # Graupel Water Path\n",
    "\n",
    "    TWP = LWP + IWP\n",
    "\n",
    "    # Use xarray.where for masking with Dask arrays\n",
    "    # Ensure LWP is not zero before division to avoid inf/nan\n",
    "    cr = xr.where(LWP != 0, IWP / LWP, np.inf) # Use np.inf for where LWP is zero, so cr_thresh condition handles it\n",
    "\n",
    "    # Threshold parameters (unchanged)\n",
    "    twp_thresh = 1e-1\n",
    "    cr_thresh = 2\n",
    "    graup_thresh = 1e-4\n",
    "    rain_thresh_conv = 1e-1\n",
    "    rain_thresh_strat = 1e-2\n",
    "\n",
    "    # Initialize output array as a Dask array of zeros\n",
    "    # Use dask.array.zeros or xarray.zeros_like to create a Dask-backed array\n",
    "    # The shape should be (Time, nCells) after the initial q_int[0] selection\n",
    "    # Assuming q_int has dimensions (variable, Time, nCells)\n",
    "    c_type_shape = LWP.shape # Should be (Time, nCells)\n",
    "    c_type = da.zeros(c_type_shape, dtype=np.int8)\n",
    "\n",
    "    # Use dask.array.where for efficient boolean indexing with Dask arrays\n",
    "    # Deep convection\n",
    "    condition_dc = ((LWP != 0) & (TWP > twp_thresh) & \\\n",
    "                    (cr <= cr_thresh) & \\\n",
    "                    (RWP >= rain_thresh_conv) & \\\n",
    "                    (GWP >= graup_thresh))\n",
    "    c_type = da.where(condition_dc, 1, c_type)\n",
    "\n",
    "    # Congestus\n",
    "    condition_cg = ((LWP != 0) & (TWP > twp_thresh) & \\\n",
    "                    (cr <= cr_thresh) & \\\n",
    "                    (RWP >= rain_thresh_conv) & \\\n",
    "                    (GWP < graup_thresh))\n",
    "    c_type = da.where(condition_cg, 2, c_type)\n",
    "\n",
    "    # Shallow\n",
    "    condition_sc = ((LWP != 0) & (TWP > twp_thresh) & \\\n",
    "                    (cr <= cr_thresh) & \\\n",
    "                    (RWP < rain_thresh_conv))\n",
    "    c_type = da.where(condition_sc, 3, c_type)\n",
    "\n",
    "    # Stratiform\n",
    "    condition_st = ((LWP != 0) & (TWP > twp_thresh) & \\\n",
    "                    (cr > cr_thresh) & \\\n",
    "                    (RWP >= rain_thresh_strat))\n",
    "    c_type = da.where(condition_st, 4, c_type)\n",
    "\n",
    "    # Anvil\n",
    "    condition_an = ((LWP != 0) & (TWP > twp_thresh) & \\\n",
    "                    (cr > cr_thresh) & \\\n",
    "                    (RWP < rain_thresh_strat))\n",
    "    c_type = da.where(condition_an, 5, c_type)\n",
    "\n",
    "    return c_type # This will return a Dask array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnose clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With help from ChatGPT\n",
    "\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "\n",
    "def mpas_label_from_adj(\n",
    "    c_type_flat,\n",
    "    adj_full,\n",
    "    mask_condition=None,\n",
    "    subset_indices=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Connected component labeling on MPAS using a precomputed adjacency (adj_full).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    c_type_flat : dask.array or np.ndarray, shape (nCells,)\n",
    "        Field for masking (e.g., c_type).\n",
    "    adj_full : scipy.sparse.csr_matrix\n",
    "        Full adjacency matrix for all MPAS cells.\n",
    "    mask_condition : callable or None\n",
    "        Function f(arr) -> bool array, defining which cells are \"active\".\n",
    "        If None, all cells are considered active.\n",
    "    subset_indices : array-like of int or None\n",
    "        Optional subset of cells to restrict analysis to.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    labels_out : np.ndarray\n",
    "        Label array (background = 0). Length = nCells if no subset,\n",
    "        else length = len(subset_indices).\n",
    "    num_features : int\n",
    "        Number of connected components found.\n",
    "    \"\"\"\n",
    "    \n",
    "    nCells = adj_full.shape[0]\n",
    "\n",
    "    # Mask condition\n",
    "    if mask_condition is None:\n",
    "        mask = da.ones_like(c_type_flat, dtype=bool)\n",
    "    else:\n",
    "        mask = mask_condition(c_type_flat)\n",
    "\n",
    "    # Compute mask into NumPy\n",
    "    mask_np = mask.compute() if isinstance(mask, da.Array) else np.asarray(mask)\n",
    "\n",
    "    if subset_indices is not None:\n",
    "        subset_indices = (\n",
    "            subset_indices.compute()\n",
    "            if isinstance(subset_indices, da.Array)\n",
    "            else np.asarray(subset_indices, dtype=int)\n",
    "        )\n",
    "        # mask_np = mask_np[subset_indices]\n",
    "        adj = adj_full[subset_indices, :][:, subset_indices]\n",
    "        nCells = len(subset_indices)\n",
    "    else:\n",
    "        adj = adj_full\n",
    "\n",
    "    # Keep only active nodes\n",
    "    active = np.where(mask_np)[0]\n",
    "    adj_masked = adj[active, :][:, active]\n",
    "\n",
    "    # Connected components\n",
    "    num_features, labels_active = connected_components(adj_masked, directed=False)\n",
    "\n",
    "    # Remap back to full (subset) length\n",
    "    labels_out = np.zeros(mask_np.shape[0], dtype=int)\n",
    "    labels_out[active] = labels_active + 1  # +1 to match ndimage.label\n",
    "\n",
    "    # labels_out is length nCells (or len(subset)), background = 0\n",
    "    sizes = np.bincount(labels_out)[1:]  # skip label 0 (background)\n",
    "\n",
    "    return labels_out, num_features, sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main driver loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get file list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of desired file times\n",
    "file_times_arr = np.arange('2000-05-01T06:00:00', '2000-05-11T06:00:00', 6, dtype='datetime64[h]')\n",
    "file_times = [file_times_arr[i].astype('datetime64[D]').astype(str)+'_'+str(file_times_arr[i]).split('T')[1].split(':')[0]+'.00.00' for i in range(len(file_times_arr))]\n",
    "\n",
    "istart_set=11\n",
    "# file_times[istart_set]\n",
    "# file_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting loop...\n",
      "Processing file: pclass_cluster_CTL_2000-05-01_06.00.00.pickle\n",
      "Finished processing CTL\n",
      "Classification complete.\n",
      "CPU times: user 31.7 s, sys: 20.3 s, total: 52.1 s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Main loop\n",
    "import pickle\n",
    "\n",
    "pclass_names = ['DC', 'CG', 'SC', 'ST', 'AN', 'DSA']\n",
    "\n",
    "print('Starting loop...')\n",
    "nCells_chunk_size = 100000\n",
    "\n",
    "exp_names = [\"CTL\", \"HOMO_RAD\", \"CLIM_RAD\"]\n",
    "\n",
    "main_path = \"/glade/campaign/mmm/dpm/rberrios/glade_scratch/MPAS_APE/aqua_sstmax10N_ASD/\"\n",
    "\n",
    "for expName in exp_names:\n",
    "# for expName in exp_names[:1]:\n",
    "\n",
    "    data_path = f\"{main_path}{expName}/TC_3km/\"\n",
    "    pickle_dir = f\"/glade/derecho/scratch/ruppert/tc-crfrad/mpas/{expName}\"\n",
    "\n",
    "    # Open the dataset with dask backend. This loads lazily.\n",
    "    # Specify chunks to optimize memory usage and parallel processing.\n",
    "    # You'll need to know typical chunk sizes for your variables, or let xarray guess.\n",
    "    # For large datasets, manual chunking can be critical.\n",
    "    # Example: If 'Time' dimension is large, chunk it. 'nCells' might be good to chunk too.\n",
    "    # ds = xr.open_mfdataset(data_path + \"waterPaths*\", combine=\"nested\", concat_dim=\"Time\",\n",
    "    #                        chunks={'Time': 'auto', 'nCells': 'auto'}) # 'auto' lets Dask guess\n",
    "    # Or specify explicitly, e.g., {'Time': 24, 'nCells': 1000}\n",
    "\n",
    "    if expName == \"CTL\":\n",
    "        istart = 0\n",
    "    elif expName == \"HOMO_RAD\":\n",
    "        istart = 0\n",
    "    elif expName == \"CLIM_RAD\":\n",
    "        istart = 0\n",
    "        # istart = istart_set\n",
    "\n",
    "    # for time in file_times:\n",
    "    for time in file_times[istart:]:\n",
    "    # for time in file_times[0:1]:\n",
    "\n",
    "        # print(f\"Processing {expName} for time {time}\")\n",
    "        print(f\"Processing file: pclass_cluster_{expName}_{time}.pickle\")\n",
    "\n",
    "        # print('opening files')\n",
    "        # wp_files = [data_path+'waterPaths.'+time+'.nc' for time in file_times]\n",
    "        wp_files = data_path+'waterPaths.'+time+'.nc'\n",
    "        # ds = xr.open_mfdataset(wp_files,\n",
    "        ds = xr.open_mfdataset(wp_files,\n",
    "                    # combine=\"nested\", concat_dim=\"Time\", \n",
    "                    parallel=True, \n",
    "                    chunks={\"Time\": -1, \"nCells\": nCells_chunk_size})\n",
    "\n",
    "        # print()\n",
    "        # print('subsetting')\n",
    "        # Select cells within latitude range. This operation is also lazy if `ds` is Dask-backed.\n",
    "        # ds_tropical = ds.isel(nCells=ind_within_lat)\n",
    "        ds_tropical = ds\n",
    "\n",
    "        # print('reading variables')\n",
    "        # Convert to a list of DataArrays, and wrap in dask.array.stack to create a single Dask array\n",
    "        # This creates a Dask-backed array 'q_int_dask' without loading data into memory yet.\n",
    "        q_int_dask = da.stack([\n",
    "            ds_tropical.lwp.data,\n",
    "            ds_tropical.iwp.data,\n",
    "            ds_tropical.rwp.data,\n",
    "            ds_tropical.gwp.data\n",
    "        ], axis=0) # Stack along a new 0th dimension for the different water paths\n",
    "\n",
    "        # print('classifying')\n",
    "        # Call the classification function. This will return a Dask array (c_type_dask).\n",
    "        # The actual computation of c_type is still lazy at this point.\n",
    "        c_type_dask = precip_class(q_int_dask)\n",
    "\n",
    "        # Now, use dask.compute to run all these sums in parallel\n",
    "        # This is the point where the data will actually be loaded and processed by Dask workers.\n",
    "        # This avoids loading the full `c_type` array into memory at once.\n",
    "        results = dask.compute(c_type_dask)[0] # dask.compute returns a tuple of results\n",
    "\n",
    "        labels_write = []\n",
    "        nfeat_write = []\n",
    "        sizes_write = []\n",
    "        for iclass in range(nclass):\n",
    "\n",
    "            if iclass < (nclass-1):\n",
    "                mask_condition = lambda arr, iclass=iclass: arr == (iclass + 1)\n",
    "            else:\n",
    "                mask_condition = lambda arr: (arr == 1) | (arr >= 4)\n",
    "\n",
    "            labels, nfeat, sizes = mpas_label_from_adj(\n",
    "                c_type_dask[0],\n",
    "                adj_full,\n",
    "                mask_condition=mask_condition,\n",
    "                subset_indices=None\n",
    "                )\n",
    "            labels_write.append(labels)\n",
    "            nfeat_write.append(nfeat)\n",
    "            sizes_write.append(sizes)\n",
    "\n",
    "        # Write out to pickle\n",
    "\n",
    "        pickle_file_out = f\"{pickle_dir}/pclass_cluster_{expName}_{time}.pickle\"\n",
    "        with open(pickle_file_out, 'wb') as f:\n",
    "            # pickle.dump(PE_thisExp, f)\n",
    "            pickle.dump([labels_write, nfeat_write, sizes_write], f)\n",
    "\n",
    "    print(f\"Finished processing {expName}\")\n",
    "\n",
    "print('Classification complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def clusters_map(c_type, xdim, ydim):\n",
    "# ind = np.where(labels != 0)\n",
    "# labels_masked = np.ma.masked_where(labels == 0, labels)\n",
    "# fig = plt.figure(figsize=(12,10))\n",
    "# fig.set_facecolor('white')\n",
    "# ax = fig.add_subplot(211)#, aspect='equal')\n",
    "# # plt.pcolormesh(lonCell, latCell, labels_masked, cmap='flag')\n",
    "# # sc = ax.scatter(lonCell[ind_within_lat], latCell[ind_within_lat], c=labels_masked, cmap=\"tab20\", s=2, marker=\"s\")\n",
    "# sc = ax.scatter(lonCell, latCell, c=labels_masked, cmap=\"tab20\", s=2, marker=\"s\")\n",
    "# ax.set_title(\"Clusters (N = \"+str(nfeat)+\")\")\n",
    "# # ax.set_xlabel('[km]')\n",
    "# # ax.set_ylabel('[km]')\n",
    "# latbounds = [10, 20.0]\n",
    "# ind_within_lat = np.where( (latCell >= latbounds[0]) & (latCell <= latbounds[1]) )[0]\n",
    "# # plt.ylim((np.min(latCell[ind_within_lat]), np.max(latCell[ind_within_lat])))\n",
    "# # plt.ylim(ylim)\n",
    "# # plt.tight_layout()\n",
    "# plt.show()\n",
    "# # plt.text(500,1750, 'Total = '+str(num_features))\n",
    "# # return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plotting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
