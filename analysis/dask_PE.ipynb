{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d67a627d-3d81-4b59-815b-56f8cf3ef058",
   "metadata": {},
   "source": [
    "Calculate and write out vmf, p-class, and PE from Rosi's MPAS simulations.\n",
    "\n",
    "Using modified code from Rosi's notebooks.\n",
    "\n",
    "This version reads in pre-processed VMF from netCDF files per experiment and time step.\n",
    "\n",
    "Optimized by gemini."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c1d3be",
   "metadata": {},
   "source": [
    "### Main settings and cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "107297ce-9b59-4ce4-aad0-e3e85bf32564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import dask\n",
    "import zarr\n",
    "import pickle\n",
    "import dask.array as da\n",
    "from precip_class_mpas import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a78e54c",
   "metadata": {},
   "source": [
    "#### Start cluster and scale it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f03a4cf-0100-438f-9560-5d4e7d1cb7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from ncar_jobqueue import NCARCluster\n",
    "# # cluster = PBSCluster()\n",
    "# # cluster.scale(4 * 9) # Ask for 4 x 9 workers\n",
    "# # cluster\n",
    "\n",
    "# from dask_jobqueue import PBSCluster\n",
    "# # Create a PBS cluster object\n",
    "# # cluster = PBSCluster(\n",
    "# #     job_name = 'dask-wk23-hpc',\n",
    "# #     cores = 1,\n",
    "# #     processes = 1,\n",
    "# #     log_directory = '/glade/derecho/scratch/rberrios/dask/',\n",
    "# #     local_directory = '/glade/derecho/scratch/rberrios/dask/',\n",
    "# #     resource_spec = 'select=1:ncpus=1:mem=10GB',\n",
    "# #     queue = 'casper',\n",
    "# #     walltime = '12:00:00',\n",
    "# #     interface = 'mgt'\n",
    "# # )\n",
    "# cluster = PBSCluster(\n",
    "#     job_name = 'dask-wk23-hpc',\n",
    "#     cores = 1,\n",
    "#     processes = 1,\n",
    "#     memory = '10GiB',\n",
    "#     account = 'UOKL0049',\n",
    "#     log_directory = '/glade/derecho/scratch/ruppert/dask/',\n",
    "#     local_directory = '/glade/derecho/scratch/ruppert/dask/',\n",
    "#     resource_spec = 'select=1:ncpus=1:mem=10GB',\n",
    "#     queue = 'casper',\n",
    "#     # queue = 'main',\n",
    "#     walltime = '12:00:00',\n",
    "#     interface = 'ext'\n",
    "# )\n",
    "# print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57234228-6b1a-4ad7-a53e-edefc0ccf414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask.distributed import Client\n",
    "# client = Client(cluster) # Connect this local process to remote workers\n",
    "# client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8824d7af-bb12-4ba7-b271-f2d1e95aa192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scale the cluster to n workers (which will use n jobs here)\n",
    "# ncpu = 36\n",
    "# # ncpu = 24\n",
    "# # ncpu = 10\n",
    "# # ncpu = 18\n",
    "# cluster.scale(ncpu)\n",
    "\n",
    "# # Block progress until workers have spawned (typically only in demos and benchmarks!)\n",
    "# client.wait_for_workers(ncpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30b5af2",
   "metadata": {},
   "source": [
    "#### Set paths, read initial conditions, find tropical indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a303978e-da2a-4b60-8e44-5515e8463db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = \"TC_3km\"\n",
    "# grid_path = \"/glade/work/rberrios/MPAS/aqua_sstmax10N_ASD/plus4K/TC_3km/x5.tropical_3km_10N.init.nc\"\n",
    "grid_pickle_file = \"/glade/campaign/univ/uokl0049/jruppert/pickle_out/grid_data.pickle\"\n",
    "with open(grid_pickle_file, 'rb') as f:\n",
    "    areaCell, latCell, lonCell = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a4ab09b-2203-4476-85d5-86e1fbeafb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid = xr.open_dataset(grid_path)\n",
    "# latCell = np.degrees(grid.latCell)\n",
    "# lonCell = np.degrees(grid.lonCell)\n",
    "# areaCell = grid.areaCell\n",
    "\n",
    "#find indexes within desired latitudinal range\n",
    "latbounds_all = [\n",
    "    [0, 15.0],\n",
    "    [10, 20.0],\n",
    "    [15, 20],\n",
    "]\n",
    "# latbounds = [15, 20.0]\n",
    "# latbounds = [0, 15.0]\n",
    "# ind_within_lat = np.where( (latCell >= latbounds[0]) & (latCell <= latbounds[1]) )[0]\n",
    "\n",
    "# areaCell_tropical = areaCell.isel(nCells=ind_within_lat)\n",
    "# len(ind_within_lat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41311849",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb3fe7f",
   "metadata": {},
   "source": [
    "#### Function to get PE, VMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "673e43e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to calculate mass-flux based precipitation efficiency ###\n",
    "def calc_massFlux_precipitationEfficiency(M_u, M_d, cape, cin, areaCell, c_type_dask):\n",
    "\n",
    "    # ntime = M_u.shape[0]\n",
    "\n",
    "    # mu_clouds = da.zeros((ntime, 6), dtype=M_u.dtype) # Assuming M_u is a Dask array\n",
    "    # md_clouds = da.zeros((ntime, 6), dtype=M_u.dtype)\n",
    "    # count_clouds = da.zeros((ntime, 6), dtype=M_u.dtype) # How many cells contribute to each cloud type\n",
    "    mu_clouds = da.zeros(6, dtype=M_u.dtype) # Assuming M_u is a Dask array\n",
    "    md_clouds = da.zeros(6, dtype=M_u.dtype)\n",
    "    cape_clouds = da.zeros(6, dtype=M_u.dtype)\n",
    "    cin_clouds = da.zeros(6, dtype=M_u.dtype)\n",
    "    count_clouds = da.zeros(6, dtype=M_u.dtype) # How many cells contribute to each cloud type\n",
    "\n",
    "    # for it in range(ntime):\n",
    "\n",
    "    for i in np.arange(1, 7): # Assuming c_type_dask ranges from 1 to 5\n",
    "    # for i in np.arange(6, 7): # Assuming c_type_dask ranges from 1 to 5\n",
    "\n",
    "        print(f\"Processing cloud type {i}\")\n",
    "\n",
    "        # Ensure mask is computed as a dask array if c_type_dask is dask\n",
    "        if i == 6:\n",
    "            mask = ((c_type_dask == 1) | (c_type_dask == 4) | (c_type_dask == 5)) # Combine DC, ST, AN\n",
    "        else:\n",
    "            mask = (c_type_dask == i)\n",
    "\n",
    "        count = mask.sum()\n",
    "\n",
    "        # epsilon_mask = epsilon_v2.where(mask)\n",
    "        Md_mask = M_d.where(mask)\n",
    "        Mu_mask = M_u.where(mask)\n",
    "        cape_mask = cape.where(mask)\n",
    "        cin_mask = cin.where(mask)\n",
    "        # areaCell_mask = da.repeat(areaCell[da.newaxis,...], M_d.shape[0]).where(mask) # areaCell should ideally be broadcastable or have matching dims\n",
    "        areaCell_mask = areaCell.where(mask) # areaCell should ideally be broadcastable or have matching dims\n",
    "\n",
    "        # It's better to compute the weighted sum and total area, then divide.\n",
    "        # This will create a dask graph. The .compute() will happen outside the loop\n",
    "        # if the list is returned, or if you explicitly call .compute() here.\n",
    "        # For a list, you'd typically gather them later.\n",
    "        # weighted_sum = (epsilon_mask * areaCell_mask).sum(dim='nCells', skipna=True)\n",
    "        total_area = areaCell_mask.sum(dim='nCells', skipna=True)\n",
    "        Mu_weighted_sum = (Mu_mask * areaCell_mask).sum(dim='nCells', skipna=True)\n",
    "        Md_weighted_sum = (Md_mask * areaCell_mask).sum(dim='nCells', skipna=True)\n",
    "        cape_weighted_sum = (cape_mask * areaCell_mask).sum(dim='nCells', skipna=True)\n",
    "        cin_weighted_sum = (cin_mask * areaCell_mask).sum(dim='nCells', skipna=True)\n",
    "\n",
    "        # Handle division by zero for total_area to avoid NaN/inf\n",
    "        # epsilon_mask_mean_v2 = (weighted_sum / total_area).where(total_area != 0)\n",
    "        Mu_mask_mean = (Mu_weighted_sum / total_area).where(total_area != 0)\n",
    "        Md_mask_mean = (Md_weighted_sum / total_area).where(total_area != 0)\n",
    "        cape_mask_mean = (cape_weighted_sum / total_area).where(total_area != 0)\n",
    "        cin_mask_mean = (cin_weighted_sum / total_area).where(total_area != 0)\n",
    "\n",
    "        # Finally, calculate epsilon for cloud type with averaged Md, Mu\n",
    "        # V1\n",
    "        # epsilon_mask_mean_v1 = 1.0 - (Md_mask_mean / Mu_mask_mean).where(Mu_mask_mean != 0) # Avoid inf/NaN where M_d is zero\n",
    "        # epsilon_clouds.append(epsilon_mask_mean_v1) # Append dask.array.Array objects\n",
    "        # count_clouds.append(count)\n",
    "        # mu_clouds.append(Mu_mask_mean)\n",
    "        # md_clouds.append(Md_mask_mean)\n",
    "        count_clouds[i-1] = count\n",
    "        mu_clouds[i-1] = Mu_mask_mean\n",
    "        md_clouds[i-1] = Md_mask_mean\n",
    "        cape_clouds[i-1] = cape_mask_mean\n",
    "        cin_clouds[i-1] = cin_mask_mean\n",
    "\n",
    "        # Put V2 after V1\n",
    "        # epsilon_clouds.append(epsilon_mask_mean_v2) # Append dask.array.Array objects\n",
    "\n",
    "    # finally, domain-mean\n",
    "    # If you want to compute the domain mean of epsilon for the whole domain (not by cloud type)\n",
    "    # domain_mean_epsilon = (epsilon * areaCell).sum(dim='nCells', skipna=True) / areaCell.sum(dim='nCells', skipna=True)\n",
    "    # You could return both epsilon_clouds and domain_mean_epsilon, or whatever makes sense for your output.\n",
    "    return [count_clouds, mu_clouds, md_clouds, cape_clouds, cin_clouds] # This will be a list of Dask DataArray "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d230b7fc",
   "metadata": {},
   "source": [
    "### Main driver loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62e5396",
   "metadata": {},
   "source": [
    "#### Get file list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4fe82a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2000-05-04_00.00.00'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of desired file times\n",
    "file_times_arr = np.arange('2000-05-01T06:00:00', '2000-05-11T06:00:00', 6, dtype='datetime64[h]')\n",
    "file_times = [file_times_arr[i].astype('datetime64[D]').astype(str)+'_'+str(file_times_arr[i]).split('T')[1].split(':')[0]+'.00.00' for i in range(len(file_times_arr))]\n",
    "\n",
    "istart_set=11\n",
    "file_times[istart_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090be225",
   "metadata": {},
   "source": [
    "#### Start loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f2eb282-8a39-45a5-8a98-95bad81ccbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting loop...\n",
      "Processing file: VMF_pclass_CTL_2000-05-01_06.00.00_0-15.0.pickle\n",
      "Opening WP files\n",
      "Opening VMF files\n",
      "Reading in CAPE\n",
      "Subsetting\n",
      "Stacking WP\n",
      "Subsetting CAPE\n",
      "Calling pclass\n",
      "Calling main PE function\n",
      "Processing cloud type 1\n",
      "Processing cloud type 2\n",
      "Processing cloud type 3\n",
      "Processing cloud type 4\n",
      "Processing cloud type 5\n",
      "Processing cloud type 6\n",
      "Computing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/ruppert/conda-envs/plotting/lib/python3.11/site-packages/dask/_task_spec.py:740: RuntimeWarning: divide by zero encountered in divide\n",
      "  return self.func(*new_argspec)\n",
      "/glade/work/ruppert/conda-envs/plotting/lib/python3.11/site-packages/dask/_task_spec.py:740: RuntimeWarning: invalid value encountered in divide\n",
      "  return self.func(*new_argspec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing CTL\n",
      "Classification complete.\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# Main loop\n",
    "import pickle\n",
    "\n",
    "pclass_names = ['DC', 'CG', 'SC', 'ST', 'AN', 'DSA']\n",
    "\n",
    "print('Starting loop...')\n",
    "nCells_chunk_size = 100000\n",
    "\n",
    "exp_names = [\"CTL\", \"HOMO_RAD\", \"CLIM_RAD\"]\n",
    "\n",
    "main_path = \"/glade/campaign/mmm/dpm/rberrios/glade_scratch/MPAS_APE/aqua_sstmax10N_ASD/\"\n",
    "pickle_dir = '/glade/campaign/univ/uokl0049/jruppert/pickle_out/'\n",
    "\n",
    "# for latbounds in latbounds_all:\n",
    "for latbounds in latbounds_all[0:1]:\n",
    "\n",
    "    ind_within_lat = np.where( (latCell >= latbounds[0]) & (latCell <= latbounds[1]) )[0]\n",
    "    areaCell_tropical = areaCell.isel(nCells=ind_within_lat)\n",
    "\n",
    "    def preprocess(ds):\n",
    "        return ds.isel(nCells=ind_within_lat)\n",
    "\n",
    "    # for expName in exp_names:\n",
    "    for expName in exp_names[0:1]:\n",
    "\n",
    "        data_path = f\"{main_path}{expName}/TC_3km/\"\n",
    "        scdir = '/glade/derecho/scratch/ruppert/tc-crfrad/mpas/'+expName+'/'\n",
    "\n",
    "        # data_path = f\"/glade/campaign/mmm/dpm/rberrios/glade_scratch/MPAS_APE/aqua_sstmax10N_ASD/{expName}/TC_3km/\"\n",
    "\n",
    "        # Open the dataset with dask backend. This loads lazily.\n",
    "        # Specify chunks to optimize memory usage and parallel processing.\n",
    "        # You'll need to know typical chunk sizes for your variables, or let xarray guess.\n",
    "        # For large datasets, manual chunking can be critical.\n",
    "        # Example: If 'Time' dimension is large, chunk it. 'nCells' might be good to chunk too.\n",
    "        # ds = xr.open_mfdataset(data_path + \"waterPaths*\", combine=\"nested\", concat_dim=\"Time\",\n",
    "        #                        chunks={'Time': 'auto', 'nCells': 'auto'}) # 'auto' lets Dask guess\n",
    "        # Or specify explicitly, e.g., {'Time': 24, 'nCells': 1000}\n",
    "\n",
    "        if expName == \"CTL\":\n",
    "            istart = 0\n",
    "        elif expName == \"HOMO_RAD\":\n",
    "            istart = 0\n",
    "        elif expName == \"CLIM_RAD\":\n",
    "            istart = 0#istart_set\n",
    "\n",
    "        # for time in file_times[istart:]:\n",
    "        for time in file_times[0:1]:\n",
    "\n",
    "            # print(f\"Processing {expName} for time {time}\")\n",
    "            print(f\"Processing file: VMF_pclass_{expName}_{time}_{str(latbounds[0])}-{str(latbounds[1])}.pickle\")\n",
    "\n",
    "            print('Opening WP files')\n",
    "            # wp_files = [data_path+'waterPaths.'+time+'.nc' for time in file_times]\n",
    "            wp_files = data_path+'waterPaths.'+time+'.nc'\n",
    "            # ds = xr.open_mfdataset(wp_files,\n",
    "            # ds = xr.open_dataset(wp_files,)\n",
    "            ds_tropical = xr.open_mfdataset(wp_files,\n",
    "                        combine=\"nested\", concat_dim=\"Time\", \n",
    "                        preprocess = preprocess,\n",
    "                        parallel=True, \n",
    "                        chunks={\"Time\": -1, \"nCells\": nCells_chunk_size})\n",
    "\n",
    "            # vmf_files = [scdir+'vmfs.'+time+'.nc' for time in file_times]\n",
    "            print('Opening VMF files')\n",
    "            vmf_files = scdir+'vmfs.'+time+'.nc'\n",
    "            # ds_vmf = xr.open_dataset(vmf_files)\n",
    "            ds_vmf_tropical = xr.open_mfdataset(vmf_files,\n",
    "                        # combine=\"nested\", concat_dim=\"Time\", \n",
    "                        preprocess = preprocess,\n",
    "                        parallel=True,\n",
    "                        chunks={\"Time\": -1, \"nCells\": nCells_chunk_size})\n",
    "\n",
    "            # READ IN CAPE FROM ZARR\n",
    "            print('Reading in CAPE')\n",
    "            cape_path = f\"{scdir}/CAPE_{time}.zarr\"\n",
    "            root = zarr.open(cape_path, mode=\"r\")\n",
    "            # Wrap Zarr arrays as Dask arrays\n",
    "            cape_da = xr.DataArray(da.from_array(root[\"CAPE\"], chunks=nCells_chunk_size), dims=(\"nCells\",))\n",
    "            cin_da  = xr.DataArray(da.from_array(root[\"CIN\"],  chunks=nCells_chunk_size), dims=(\"nCells\",))\n",
    "            # cape_da = xr.DataArray(root[\"CAPE\"], dims=(\"nCells\",))\n",
    "            # cin_da  = xr.DataArray(root[\"CIN\"], dims=(\"nCells\",))\n",
    "\n",
    "            # Subset datasets\n",
    "            print('Subsetting')\n",
    "            # print('subsetting')\n",
    "            # Select cells within latitude range. This operation is also lazy if `ds` is Dask-backed.\n",
    "            # ds_tropical = ds.isel(nCells=ind_within_lat)\n",
    "            # ds_vmf_tropical = ds_vmf.isel(nCells=ind_within_lat)\n",
    "\n",
    "            # print('reading variables')\n",
    "            # Convert to a list of DataArrays, and wrap in dask.array.stack to create a single Dask array\n",
    "            # This creates a Dask-backed array 'q_int_dask' without loading data into memory yet.\n",
    "            print('Stacking WP')\n",
    "            q_int_dask = da.stack([\n",
    "                ds_tropical.lwp.data,\n",
    "                ds_tropical.iwp.data,\n",
    "                ds_tropical.rwp.data,\n",
    "                ds_tropical.gwp.data\n",
    "            ], axis=0) # Stack along a new 0th dimension for the different water paths\n",
    "\n",
    "            mu = ds_vmf_tropical.mu\n",
    "            md = ds_vmf_tropical.md\n",
    "\n",
    "            # Subset and tidy up CAPE, CIN to cooperate with VMF function\n",
    "            print('Subsetting CAPE')\n",
    "            cape_da = cape_da.isel(nCells=ind_within_lat)\n",
    "            cin_da  = cin_da.isel(nCells=ind_within_lat)\n",
    "            cape_da = cape_da.expand_dims(\"Time\")\n",
    "            cin_da  = cin_da.expand_dims(\"Time\")\n",
    "            # cape_da = cape_da.chunk({\"Time\": 1, \"nCells\": nCells_chunk_size})\n",
    "            # cin_da  = cin_da.chunk({\"Time\": 1, \"nCells\": nCells_chunk_size})\n",
    "\n",
    "            # print('classifying')\n",
    "            # Call the classification function. This will return a Dask array (c_type_dask).\n",
    "            # The actual computation of c_type is still lazy at this point.\n",
    "            print('Calling pclass')\n",
    "            c_type_dask = precip_class_mpas(q_int_dask)\n",
    "\n",
    "            # print('getting PE')\n",
    "            # Get Mu, Md as a function of PClass\n",
    "            print('Calling main PE function')\n",
    "            vmf_pclass = calc_massFlux_precipitationEfficiency(mu, md, cape_da, cin_da, areaCell_tropical, c_type_dask[0])\n",
    "\n",
    "            # print('daks.compute for VMFs')\n",
    "            print('Computing')\n",
    "            results = dask.compute(vmf_pclass)[0] # dask.compute returns a tuple of results\n",
    "            count_results = results[0]\n",
    "            mu_results    = results[1]\n",
    "            md_results    = results[2]\n",
    "            cape_results  = results[3]\n",
    "            cin_results   = results[4]\n",
    "\n",
    "            # Write out to pickle\n",
    "\n",
    "            # # pickle_file_out = f\"{scdir}PE_massFlux_{expName}_{time}.pickle\"\n",
    "            # pickle_file_out = f\"{pickle_dir}VMF_pclass_{expName}_{time}_{str(latbounds[0])}-{str(latbounds[1])}.pickle\"\n",
    "            # with open(pickle_file_out, 'wb') as f:\n",
    "            #     # pickle.dump(PE_thisExp, f)\n",
    "            #     pickle.dump([count_results, mu_results, md_results, cape_results, cin_results], f)\n",
    "\n",
    "        print(f\"Finished processing {expName}\")\n",
    "\n",
    "    print('Classification complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f1b07c",
   "metadata": {},
   "source": [
    "### Close cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3ff3722-87e9-4d64-a6ab-5ff9aa1ec943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plotting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
